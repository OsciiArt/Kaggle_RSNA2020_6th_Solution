{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob, pickle, time, gc, copy, sys\n",
    "import warnings\n",
    "import pydicom\n",
    "import multiprocessing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sys.path.append('../src')\n",
    "from utils import ri, pickle_load, pickle_save\n",
    "from preprocess_fold import data_split_StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from sklearn import metrics\n",
    "from utils_pytorch import cycle, CosineLR, AverageMeter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    RandomCrop,\n",
    "    RandomBrightnessContrast,\n",
    "    ShiftScaleRotate,\n",
    "    Cutout,\n",
    ")\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "DEBUG = False # set False to do all process\n",
    "output_dir = \"../output\"\n",
    "preprocess_dir = \"../input/preprocess\"\n",
    "df_train_path = \"{}/df_train.csv\".format(preprocess_dir)\n",
    "train_npy_dir_path = \"{}/train_law_npy\".format(preprocess_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLD = 5\n",
    "NUM_EPOCH = 8\n",
    "NUM_CYCLE = 4\n",
    "BATCH_SIZE = 64\n",
    "FOLD_LIST = [1,2,3,4,5]\n",
    "LR_RANGE = [1e-3, 1e-5]\n",
    "STEP_PER_EPOCH = 256\n",
    "MODEL_NAME = 'b0' # b0 or b2\n",
    "if DEBUG:\n",
    "    NUM_EPOCH = 1\n",
    "    NUM_CYCLE = 1\n",
    "    FOLD_LIST = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASS: 10\n"
     ]
    }
   ],
   "source": [
    "col_index = 'SOPInstanceUID'\n",
    "col_groupby = 'StudyInstanceUID'\n",
    "col_targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "    'pe_present_on_image',\n",
    "]\n",
    "NUM_CLASS = len(col_targets)\n",
    "print('NUM_CLASS: {}'.format(NUM_CLASS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1790594, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "      <th>negative_exam_for_pe</th>\n",
       "      <th>qa_motion</th>\n",
       "      <th>qa_contrast</th>\n",
       "      <th>flow_artifact</th>\n",
       "      <th>rv_lv_ratio_gte_1</th>\n",
       "      <th>rv_lv_ratio_lt_1</th>\n",
       "      <th>leftsided_pe</th>\n",
       "      <th>chronic_pe</th>\n",
       "      <th>true_filling_defect_not_pe</th>\n",
       "      <th>rightsided_pe</th>\n",
       "      <th>acute_and_chronic_pe</th>\n",
       "      <th>central_pe</th>\n",
       "      <th>indeterminate</th>\n",
       "      <th>dicom_path</th>\n",
       "      <th>RescaleSlope</th>\n",
       "      <th>RescaleIntercept</th>\n",
       "      <th>PatientPosition</th>\n",
       "      <th>series_index</th>\n",
       "      <th>num_series</th>\n",
       "      <th>m_i</th>\n",
       "      <th>q_i</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>exam_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>baedb900c69c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.33871</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>52b6b0b793bb</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.33871</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>1997c99c9d59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.33871</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>c6f29ac6659b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.33871</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>487d9ab5531f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.33871</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   baedb900c69c                  0.0   \n",
       "1     6897fa9de148      2bfbb7fd2e8b   52b6b0b793bb                  0.0   \n",
       "2     6897fa9de148      2bfbb7fd2e8b   1997c99c9d59                  0.0   \n",
       "3     6897fa9de148      2bfbb7fd2e8b   c6f29ac6659b                  0.0   \n",
       "4     6897fa9de148      2bfbb7fd2e8b   487d9ab5531f                  0.0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                   0.0        0.0          0.0            0.0   \n",
       "1                   0.0        0.0          0.0            0.0   \n",
       "2                   0.0        0.0          0.0            0.0   \n",
       "3                   0.0        0.0          0.0            0.0   \n",
       "4                   0.0        0.0          0.0            0.0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  leftsided_pe  chronic_pe  \\\n",
       "0                0.0               1.0           1.0         0.0   \n",
       "1                0.0               1.0           1.0         0.0   \n",
       "2                0.0               1.0           1.0         0.0   \n",
       "3                0.0               1.0           1.0         0.0   \n",
       "4                0.0               1.0           1.0         0.0   \n",
       "\n",
       "   true_filling_defect_not_pe  rightsided_pe  acute_and_chronic_pe  \\\n",
       "0                         0.0            1.0                   0.0   \n",
       "1                         0.0            1.0                   0.0   \n",
       "2                         0.0            1.0                   0.0   \n",
       "3                         0.0            1.0                   0.0   \n",
       "4                         0.0            1.0                   0.0   \n",
       "\n",
       "   central_pe  indeterminate  \\\n",
       "0         0.0            0.0   \n",
       "1         0.0            0.0   \n",
       "2         0.0            0.0   \n",
       "3         0.0            0.0   \n",
       "4         0.0            0.0   \n",
       "\n",
       "                                          dicom_path  RescaleSlope  \\\n",
       "0  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "1  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "2  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "3  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "4  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "\n",
       "   RescaleIntercept PatientPosition  series_index  num_series   m_i      q_i  \\\n",
       "0           -1024.0             HFS           0.0         124  42.0  0.33871   \n",
       "1           -1024.0             HFS           1.0         124  42.0  0.33871   \n",
       "2           -1024.0             HFS           2.0         124  42.0  0.33871   \n",
       "3           -1024.0             HFS           3.0         124  42.0  0.33871   \n",
       "4           -1024.0             HFS           4.0         124  42.0  0.33871   \n",
       "\n",
       "                                            npy_path  exam_index  \n",
       "0  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  \n",
       "1  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  \n",
       "2  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  \n",
       "3  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  \n",
       "4  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train data\n",
    "df_train = pd.read_csv(df_train_path)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7279, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "      <th>negative_exam_for_pe</th>\n",
       "      <th>qa_motion</th>\n",
       "      <th>qa_contrast</th>\n",
       "      <th>flow_artifact</th>\n",
       "      <th>rv_lv_ratio_gte_1</th>\n",
       "      <th>rv_lv_ratio_lt_1</th>\n",
       "      <th>leftsided_pe</th>\n",
       "      <th>chronic_pe</th>\n",
       "      <th>true_filling_defect_not_pe</th>\n",
       "      <th>rightsided_pe</th>\n",
       "      <th>acute_and_chronic_pe</th>\n",
       "      <th>central_pe</th>\n",
       "      <th>indeterminate</th>\n",
       "      <th>dicom_path</th>\n",
       "      <th>RescaleSlope</th>\n",
       "      <th>RescaleIntercept</th>\n",
       "      <th>PatientPosition</th>\n",
       "      <th>series_index</th>\n",
       "      <th>num_series</th>\n",
       "      <th>m_i</th>\n",
       "      <th>q_i</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>exam_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>baedb900c69c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>013358b540bb</td>\n",
       "      <td>2805267980e7</td>\n",
       "      <td>c0e16bbe96ef</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0cee26703028</td>\n",
       "      <td>bac7becd2970</td>\n",
       "      <td>64bf37f1a302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c28f3d01b14f</td>\n",
       "      <td>7d17c72fd0ce</td>\n",
       "      <td>265035a0f7aa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c8fbf1e08ac5</td>\n",
       "      <td>275497911f02</td>\n",
       "      <td>7a33bbba8386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   baedb900c69c                  0.0   \n",
       "1     013358b540bb      2805267980e7   c0e16bbe96ef                  0.0   \n",
       "2     0cee26703028      bac7becd2970   64bf37f1a302                  0.0   \n",
       "3     c28f3d01b14f      7d17c72fd0ce   265035a0f7aa                  0.0   \n",
       "4     c8fbf1e08ac5      275497911f02   7a33bbba8386                  0.0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                   0.0        0.0          0.0            0.0   \n",
       "1                   1.0        0.0          0.0            0.0   \n",
       "2                   0.0        0.0          0.0            0.0   \n",
       "3                   0.0        0.0          0.0            0.0   \n",
       "4                   1.0        0.0          0.0            0.0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  leftsided_pe  chronic_pe  \\\n",
       "0                0.0               1.0           1.0         0.0   \n",
       "1                0.0               0.0           0.0         0.0   \n",
       "2                0.0               1.0           0.0         1.0   \n",
       "3                0.0               1.0           1.0         0.0   \n",
       "4                0.0               0.0           0.0         0.0   \n",
       "\n",
       "   true_filling_defect_not_pe  rightsided_pe  acute_and_chronic_pe  \\\n",
       "0                         0.0            1.0                   0.0   \n",
       "1                         0.0            0.0                   0.0   \n",
       "2                         0.0            1.0                   0.0   \n",
       "3                         0.0            0.0                   0.0   \n",
       "4                         0.0            0.0                   0.0   \n",
       "\n",
       "   central_pe  indeterminate  \\\n",
       "0         0.0            0.0   \n",
       "1         0.0            0.0   \n",
       "2         0.0            0.0   \n",
       "3         0.0            0.0   \n",
       "4         0.0            0.0   \n",
       "\n",
       "                                          dicom_path  RescaleSlope  \\\n",
       "0  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "1  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "2  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "3  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "4  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "\n",
       "   RescaleIntercept PatientPosition  series_index  num_series   m_i       q_i  \\\n",
       "0           -1024.0             HFS           0.0         124  42.0  0.338710   \n",
       "1           -1024.0             HFS           0.0         145   0.0  0.000000   \n",
       "2           -1024.0             HFS           0.0         144  17.0  0.118056   \n",
       "3           -1024.0             HFS           0.0         105   6.0  0.057143   \n",
       "4           -1024.0             HFS           0.0          81   0.0  0.000000   \n",
       "\n",
       "                                            npy_path  exam_index  \n",
       "0  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0  \n",
       "1  /mnt/disks/data6/rsna2020/preprocessed/train_l...           1  \n",
       "2  /mnt/disks/data6/rsna2020/preprocessed/train_l...           2  \n",
       "3  /mnt/disks/data6/rsna2020/preprocessed/train_l...           3  \n",
       "4  /mnt/disks/data6/rsna2020/preprocessed/train_l...           4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make exam-level train data\n",
    "df_train_exam = ri(df_train[df_train[col_groupby].duplicated()==False])\n",
    "print(df_train_exam.shape)\n",
    "df_train_exam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "      <th>negative_exam_for_pe</th>\n",
       "      <th>qa_motion</th>\n",
       "      <th>qa_contrast</th>\n",
       "      <th>flow_artifact</th>\n",
       "      <th>rv_lv_ratio_gte_1</th>\n",
       "      <th>rv_lv_ratio_lt_1</th>\n",
       "      <th>leftsided_pe</th>\n",
       "      <th>chronic_pe</th>\n",
       "      <th>true_filling_defect_not_pe</th>\n",
       "      <th>rightsided_pe</th>\n",
       "      <th>acute_and_chronic_pe</th>\n",
       "      <th>central_pe</th>\n",
       "      <th>indeterminate</th>\n",
       "      <th>dicom_path</th>\n",
       "      <th>RescaleSlope</th>\n",
       "      <th>RescaleIntercept</th>\n",
       "      <th>PatientPosition</th>\n",
       "      <th>series_index</th>\n",
       "      <th>num_series</th>\n",
       "      <th>m_i</th>\n",
       "      <th>q_i</th>\n",
       "      <th>npy_path</th>\n",
       "      <th>exam_index</th>\n",
       "      <th>start_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>baedb900c69c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>013358b540bb</td>\n",
       "      <td>2805267980e7</td>\n",
       "      <td>c0e16bbe96ef</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0cee26703028</td>\n",
       "      <td>bac7becd2970</td>\n",
       "      <td>64bf37f1a302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.118056</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>2</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c28f3d01b14f</td>\n",
       "      <td>7d17c72fd0ce</td>\n",
       "      <td>265035a0f7aa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>3</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c8fbf1e08ac5</td>\n",
       "      <td>275497911f02</td>\n",
       "      <td>7a33bbba8386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/mnt/disks/data4/rsna2020/preprocessed/train-j...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>HFS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>/mnt/disks/data6/rsna2020/preprocessed/train_l...</td>\n",
       "      <td>4</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   baedb900c69c                  0.0   \n",
       "1     013358b540bb      2805267980e7   c0e16bbe96ef                  0.0   \n",
       "2     0cee26703028      bac7becd2970   64bf37f1a302                  0.0   \n",
       "3     c28f3d01b14f      7d17c72fd0ce   265035a0f7aa                  0.0   \n",
       "4     c8fbf1e08ac5      275497911f02   7a33bbba8386                  0.0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                   0.0        0.0          0.0            0.0   \n",
       "1                   1.0        0.0          0.0            0.0   \n",
       "2                   0.0        0.0          0.0            0.0   \n",
       "3                   0.0        0.0          0.0            0.0   \n",
       "4                   1.0        0.0          0.0            0.0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  leftsided_pe  chronic_pe  \\\n",
       "0                0.0               1.0           1.0         0.0   \n",
       "1                0.0               0.0           0.0         0.0   \n",
       "2                0.0               1.0           0.0         1.0   \n",
       "3                0.0               1.0           1.0         0.0   \n",
       "4                0.0               0.0           0.0         0.0   \n",
       "\n",
       "   true_filling_defect_not_pe  rightsided_pe  acute_and_chronic_pe  \\\n",
       "0                         0.0            1.0                   0.0   \n",
       "1                         0.0            0.0                   0.0   \n",
       "2                         0.0            1.0                   0.0   \n",
       "3                         0.0            0.0                   0.0   \n",
       "4                         0.0            0.0                   0.0   \n",
       "\n",
       "   central_pe  indeterminate  \\\n",
       "0         0.0            0.0   \n",
       "1         0.0            0.0   \n",
       "2         0.0            0.0   \n",
       "3         0.0            0.0   \n",
       "4         0.0            0.0   \n",
       "\n",
       "                                          dicom_path  RescaleSlope  \\\n",
       "0  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "1  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "2  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "3  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "4  /mnt/disks/data4/rsna2020/preprocessed/train-j...           1.0   \n",
       "\n",
       "   RescaleIntercept PatientPosition  series_index  num_series   m_i       q_i  \\\n",
       "0           -1024.0             HFS           0.0         124  42.0  0.338710   \n",
       "1           -1024.0             HFS           0.0         145   0.0  0.000000   \n",
       "2           -1024.0             HFS           0.0         144  17.0  0.118056   \n",
       "3           -1024.0             HFS           0.0         105   6.0  0.057143   \n",
       "4           -1024.0             HFS           0.0          81   0.0  0.000000   \n",
       "\n",
       "                                            npy_path  exam_index  start_index  \n",
       "0  /mnt/disks/data6/rsna2020/preprocessed/train_l...           0            0  \n",
       "1  /mnt/disks/data6/rsna2020/preprocessed/train_l...           1          124  \n",
       "2  /mnt/disks/data6/rsna2020/preprocessed/train_l...           2          269  \n",
       "3  /mnt/disks/data6/rsna2020/preprocessed/train_l...           3          413  \n",
       "4  /mnt/disks/data6/rsna2020/preprocessed/train_l...           4          518  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_exam['start_index'] = df_train[df_train[col_groupby].duplicated()==False].index.values\n",
    "df_train_exam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1790594, 11)\n",
      "(7279, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>fold1_train</th>\n",
       "      <th>fold1_valid</th>\n",
       "      <th>fold2_train</th>\n",
       "      <th>fold2_valid</th>\n",
       "      <th>fold3_train</th>\n",
       "      <th>fold3_valid</th>\n",
       "      <th>fold4_train</th>\n",
       "      <th>fold4_valid</th>\n",
       "      <th>fold5_train</th>\n",
       "      <th>fold5_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>013358b540bb</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0cee26703028</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c28f3d01b14f</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c8fbf1e08ac5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudyInstanceUID  fold1_train  fold1_valid  fold2_train  fold2_valid  \\\n",
       "0     6897fa9de148            1            0            0            1   \n",
       "1     013358b540bb            0            1            1            0   \n",
       "2     0cee26703028            1            0            1            0   \n",
       "3     c28f3d01b14f            1            0            1            0   \n",
       "4     c8fbf1e08ac5            0            1            1            0   \n",
       "\n",
       "   fold3_train  fold3_valid  fold4_train  fold4_valid  fold5_train  \\\n",
       "0            1            0            1            0            1   \n",
       "1            1            0            1            0            1   \n",
       "2            0            1            1            0            1   \n",
       "3            1            0            1            0            0   \n",
       "4            1            0            1            0            1   \n",
       "\n",
       "   fold5_valid  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            1  \n",
       "4            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data splitting stratified by 3 classes, PE positive, PE negative or indeterminate\n",
    "df_tmp = df_train_exam\n",
    "df_tmp['3class'] = 1 # PE positive\n",
    "df_tmp['3class'][df_tmp['negative_exam_for_pe']==1] = 0 # PE negative\n",
    "df_tmp['3class'][df_tmp['indeterminate']==1] = 2 # indeterminate\n",
    "df_fold_exam = data_split_StratifiedKFold(df_tmp, col_groupby, col_stratified='3class') # apply stratified K-fold\n",
    "df_fold = pd.merge(df_train[[col_groupby]], df_fold_exam, on=col_groupby, how='left')\n",
    "print(df_fold.shape)\n",
    "print(df_fold_exam.shape)\n",
    "df_fold_exam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Conv1d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv1d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "    \n",
    "class CNN_1D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=400, input_ch=1, verbose=False):\n",
    "\n",
    "        super(CNN_1D, self).__init__()\n",
    "        pool = 4\n",
    "        drop = 0.1\n",
    "        self.verbose = verbose\n",
    "        self.layer1 = nn.Sequential(\n",
    "                nn.Conv1d(input_ch//pool, 64, kernel_size=7, stride=1, padding=3, bias=False),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                SEModule(64, 16),\n",
    "        )\n",
    "        self.fpool = nn.MaxPool1d(kernel_size=pool, stride=pool, padding=0)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer2 = nn.Sequential(\n",
    "                nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                SEModule(128, 16),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "                nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                SEModule(256, 16),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "                nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                SEModule(512, 16),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc2 = nn.Conv1d(\n",
    "            input_ch//pool+64+128+256+512, \n",
    "            2, kernel_size=1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(512, 9),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        bs, ch, d = x_input.size()\n",
    "        x0 = torch.transpose(x_input, 1, 2)\n",
    "        x0 = self.fpool(x0)\n",
    "        x0 = torch.transpose(x0, 1, 2)\n",
    "        x1 = self.layer1(x0)\n",
    "        x1 = self.maxpool(x1)\n",
    "\n",
    "        x2 = self.layer2(x1)\n",
    "        x2 = self.maxpool(x2)\n",
    "        x3 = self.layer3(x2)\n",
    "        x3 = self.maxpool(x3)\n",
    "        x4 = self.layer4(x3)\n",
    "        y = self.avgpool(x4)\n",
    "        y = y.view(bs, -1)\n",
    "        y = self.fc(y)\n",
    "        \n",
    "        x5 = torch.cat([\n",
    "            x0,\n",
    "            F.adaptive_avg_pool1d(x1, d), \n",
    "            F.adaptive_avg_pool1d(x2, d), \n",
    "            F.adaptive_avg_pool1d(x3, d), \n",
    "            F.adaptive_avg_pool1d(x4, d), \n",
    "        ], axis=1)\n",
    "        y2 = self.fc2(x5)\n",
    "            \n",
    "        return y, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X_exam, X_image, shuffle=False, crop=64, verbose=False, dropout=False, cropaug=0, flip=0, fold=1):\n",
    "        self.X_exam = X_exam\n",
    "        self.X_image = X_image\n",
    "        self.shuffle = shuffle\n",
    "        self.crop = crop\n",
    "        self.verbose = verbose\n",
    "        self.cropaug = cropaug # do extra crop augment or not\n",
    "        self.flip = flip # do flip augmenet or not\n",
    "        self.feature_dir = \"{}/features_{}_fold{}\".format(output_dir, MODEL_NAME, fold)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        exam =  self.X_exam[col_groupby][index] # get the name of the selected exam\n",
    "        num_series = self.X_exam['num_series'][index] # get the num of series in the selected exam\n",
    "        \n",
    "        # crop the data\n",
    "        start_index = self.X_exam['start_index'][index]\n",
    "        end_index = start_index + num_series\n",
    "        if self.crop!=-1 and num_series>self.crop: # random crop in z-axis\n",
    "            depth = self.crop\n",
    "            tmp_index = np.random.randint(num_series-self.crop)\n",
    "            df_tmp = ri(self.X_image.iloc[start_index+tmp_index:start_index+tmp_index+self.crop])\n",
    "        else: # do not crop\n",
    "            df_tmp = ri(self.X_image.iloc[start_index:end_index])\n",
    "            depth = num_series\n",
    "            tmp_index = 0\n",
    "            \n",
    "        # load features\n",
    "        x_input = np.load(\"{}/{}_feature.npy\".format(self.feature_dir, exam))[tmp_index:tmp_index+depth]\n",
    "        x_input = x_input.astype(np.float32).transpose(1,0)\n",
    "\n",
    "        # get targets\n",
    "        # get the exam-level targets of the selected exam\n",
    "        target1 = self.X_exam[col_targets[:-1]].values[index].astype(np.float32)\n",
    "        # get the image-level targets of the selected exam\n",
    "        target2 = df_tmp[col_targets[-1]].values.astype(np.float32).reshape([1, -1])\n",
    "        # get the q_i weigh of the selected exam\n",
    "        q_weight = self.X_exam['q_i'].values[index].astype(np.float32).reshape([1])\n",
    "        \n",
    "        # do padding if length of exam data is shoreter than the defined length\n",
    "        if x_input.shape[1]<self.crop and self.crop!=-1:\n",
    "            x_input_base = np.zeros([x_input.shape[0], self.crop], np.float32)\n",
    "            x_input_base[:, :x_input.shape[1]] = x_input\n",
    "            x_input = x_input_base\n",
    "            target2_base = np.zeros([1, self.crop], np.float32)\n",
    "            target2_base[:,:target2.shape[1]] = target2\n",
    "            target2 = target2_base\n",
    "            \n",
    "        # do padding to make length multiple of 64\n",
    "        if self.crop==-1:\n",
    "            len_pad = int(np.ceil(x_input.shape[1]/64)*64)\n",
    "            x_input_base = np.zeros([x_input.shape[0], len_pad], np.float32)\n",
    "            x_input_base[:, :x_input.shape[1]] = x_input\n",
    "            x_input = x_input_base\n",
    "            target2_base = np.zeros([1, len_pad], np.float32)\n",
    "            target2_base[:,:target2.shape[1]] = target2\n",
    "            target2 = target2_base\n",
    "            \n",
    "        # do extra crop augment (crop both edges and do padding)\n",
    "        if self.cropaug>0:\n",
    "            l = np.random.randint(self.cropaug)\n",
    "            r = np.random.randint(self.cropaug)\n",
    "            x_input[:, :l] = 0\n",
    "            x_input[:, -r:] = 0\n",
    "            target2[:, :l] = 0\n",
    "            target2[:, -r:] = 0\n",
    "\n",
    "        # do flip augment\n",
    "        if np.random.rand()<self.flip:\n",
    "            x_input = np.copy(x_input[:,::-1])\n",
    "            target2 = np.copy(target2[:,::-1])\n",
    "            \n",
    "        return x_input, (target1, target2, q_weight, num_series)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set exam-level class weights\n",
    "class_weights = np.array([\n",
    "    0.0736196319, \n",
    "    0.09202453988, \n",
    "    0.1042944785, \n",
    "    0.1042944785, \n",
    "    0.1877300613, \n",
    "    0.06257668712, \n",
    "    0.06257668712,\n",
    "    0.2346625767,\n",
    "    0.0782208589,\n",
    "], np.float32)\n",
    "\n",
    "class_weights = torch.from_numpy(class_weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training\n",
    "def train(loader, model, optimizer, scheduler, num_step=-1, verbose=10):\n",
    "    if num_step==-1: num_step = len(loader)\n",
    "    loss1_avr = AverageMeter()\n",
    "    loss2_avr = AverageMeter()\n",
    "    loss3_avr = AverageMeter()\n",
    "    \n",
    "    criterion1 = nn.BCELoss().cuda()\n",
    "    criterion2 = nn.BCELoss(reduce=False).cuda()\n",
    "    lastfunc = nn.Sigmoid().cuda()\n",
    "\n",
    "    # switch to train mode\n",
    "    model .train()\n",
    "\n",
    "    starttime = time.time()\n",
    "    # training\n",
    "    itr = cycle(loader)\n",
    "    for i in range(num_step):\n",
    "        if i+1>num_step: break\n",
    "        # load batch data\n",
    "        input1, (target1, target2, q_weight, _) = next(itr)\n",
    "        input1 = torch.autograd.Variable(input1.cuda()) # input feature sequence\n",
    "        target1 = torch.autograd.Variable(target1.cuda()) # exam-level targets\n",
    "        target2 = torch.autograd.Variable(target2.cuda()) # image-level targets\n",
    "        q_weight = torch.autograd.Variable(q_weight.cuda()) # q weight of image\n",
    "        lower_limit = torch.from_numpy(np.ones([input1.size(0), 1], np.float32)*1e-3).cuda() # lower limit of q_i\n",
    "        q_weight = torch.max(q_weight, lower_limit) # apply lower limit to q weight\n",
    "\n",
    "        # compute output\n",
    "        output1, output23 = model(input1)\n",
    "        output2 = output23[:,:-1]\n",
    "        output3 = output23[:,-1:]\n",
    "        output1 = lastfunc(output1) # exam-level output [bs, 9]\n",
    "        output2 = lastfunc(output2) # image-level output [bs, 1, depth]\n",
    "        output3 = lastfunc(output3) # image-level output for q weighted loss [bs, 1, depth]\n",
    "\n",
    "        # calc losses\n",
    "        # exam-level weighted BCE\n",
    "        loss1 = criterion2(output1, target1)\n",
    "        loss1 = torch.mean(loss1, axis=0)\n",
    "        loss1 = torch.sum(loss1 * class_weights)\n",
    "        # image-level BCE\n",
    "        loss2 = criterion1(output2, target2)\n",
    "        # image-level q-weighted BCE\n",
    "        loss3 = criterion2(output3, target2)\n",
    "        loss3 = torch.mean(loss3, axis=(2))\n",
    "        loss3 = torch.mean(loss3 * q_weight)/0.053915069524371764 # 0.053915069524371764 is the mean q weight of train data\n",
    "        # total loss\n",
    "        loss = loss1*0.45 + loss2 * 0.45 + loss3*0.1 \n",
    "        \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # record losses\n",
    "        loss1_avr.update(loss1.data, input1.size(0))\n",
    "        loss2_avr.update(loss2.data, input1.size(0))\n",
    "        loss3_avr.update(loss3.data, input1.size(0))\n",
    "        \n",
    "        # display log\n",
    "        if (i+1)%verbose==0:\n",
    "            print(\"Step: {}/{} \".format(i + 1, num_step)\n",
    "                  + \"Loss 1: {:.3f} \".format(loss1_avr.avg.item())\n",
    "                  + \"Loss 2: {:.3f} \".format(loss2_avr.avg.item())\n",
    "                  + \"Loss 3: {:.3f} \".format(loss3_avr.avg.item())\n",
    "                  + \"Sec: {:.1f} \".format(time.time()-starttime)\n",
    "                  )\n",
    "            \n",
    "    return loss1_avr.avg.item() , loss2_avr.avg.item(), loss3_avr.avg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, model, verbose=10):\n",
    "    loss1_avr = AverageMeter()\n",
    "    loss2_avr = AverageMeter()\n",
    "    loss3_avr = AverageMeter()\n",
    "\n",
    "    criterion1 = nn.BCELoss().cuda()\n",
    "    criterion2 = nn.BCELoss(reduce=False).cuda() #.half()\n",
    "    lastfunc = nn.Sigmoid()\n",
    "\n",
    "    # switch to eval mode\n",
    "    model .eval()\n",
    "\n",
    "    starttime = time.time()\n",
    "    # validation\n",
    "    preds = np.zeros([0, NUM_CLASS-1], np.float32)\n",
    "    preds2 = np.zeros([0, 1], np.float32)\n",
    "    preds3 = np.zeros([0, 1], np.float32)\n",
    "    y_true = np.zeros([0, NUM_CLASS-1], np.float32)\n",
    "    y_true2 = np.zeros([0, 1], np.float32)\n",
    "    for i, (input1, (target1, target2, q_weight, num_series)) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            # load batch data\n",
    "            input1 = torch.autograd.Variable(input1.cuda())\n",
    "            target1 = torch.autograd.Variable(target1.cuda())\n",
    "            target2 = torch.autograd.Variable(target2.cuda())[:, :, :num_series]\n",
    "            q_weight = torch.autograd.Variable(q_weight.cuda())\n",
    "\n",
    "            output1, output23 = model(input1)\n",
    "            output2 = output23[:,:-1]\n",
    "            output3 = output23[:,-1:]\n",
    "            output1 = lastfunc(output1)\n",
    "            output2 = lastfunc(output2)[:, :, :num_series]\n",
    "            output3 = lastfunc(output3)[:, :, :num_series]\n",
    "            \n",
    "            # calc losses\n",
    "            loss1 = criterion2(output1, target1)\n",
    "            loss1 = torch.mean(loss1, axis=0)\n",
    "            loss1 = torch.sum(loss1 * class_weights)\n",
    "            # image-level BCE\n",
    "            loss2 = criterion1(output2, target2)\n",
    "            # image-level q-weighted BCE\n",
    "            loss3 = criterion2(output3, target2)\n",
    "            loss3 = torch.mean(loss3, axis=(2))\n",
    "            loss3 = torch.mean(loss3 * q_weight)/0.053915069524371764 \n",
    "            \n",
    "        # record losses\n",
    "        loss1_avr.update(loss1.data, input1.size(0))\n",
    "        loss2_avr.update(loss2.data, input1.size(0))\n",
    "        loss3_avr.update(loss3.data, input1.size(0))\n",
    "\n",
    "        # record pred and true data\n",
    "        # exam-level pred\n",
    "        preds = np.concatenate([preds, output1.data.cpu().numpy()]) \n",
    "        # image-level pred\n",
    "        preds2 = np.concatenate([preds2, output2.data.cpu().numpy().reshape([-1, 1])])\n",
    "        # image-level pred optimized to q-weighted BCE\n",
    "        preds3 = np.concatenate([preds3, output3.data.cpu().numpy().reshape([-1, 1])])\n",
    "        # exam-level true data\n",
    "        y_true = np.concatenate([y_true, target1.data.cpu().numpy()])\n",
    "        # image-level true data\n",
    "        y_true2 = np.concatenate([y_true2, target2.data.cpu().numpy().reshape([-1, 1])])\n",
    "        \n",
    "        # display log\n",
    "        if (i+1)%verbose==0:\n",
    "            print(\"Step: {}/{} \".format(i + 1, len(loader))\n",
    "                  + \"Loss 1: {:.3f} \".format(loss1_avr.avg.item())\n",
    "                  + \"Loss 2: {:.3f} \".format(loss2_avr.avg.item())\n",
    "                  + \"Loss 3: {:.3f} \".format(loss3_avr.avg.item())\n",
    "                  + \"Sec: {:.1f} \".format(time.time()-starttime)\n",
    "                  )\n",
    "        \n",
    "    return loss1_avr.avg.item(), loss2_avr.avg.item(), loss3_avr.avg.item(), preds, preds2, preds3, y_true, y_true2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set train log\n",
    "log_columns = ['epoch', 'loss1', 'loss2', 'loss3', 'val_loss1', 'val_loss2', 'val_loss3', 'time']\n",
    "for col in col_targets:\n",
    "    log_columns.append(\"val_bce_{}\".format(col))\n",
    "    log_columns.append(\"val_auc_{}\".format(col))\n",
    "log_columns.append(\"val_bce_q_{}\".format(col))\n",
    "log_columns.append(\"val_auc_q_{}\".format(col))\n",
    "log_columns.append(\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "Step: 100/256 Loss 1: 0.258 Loss 2: 0.110 Loss 3: 0.266 Sec: 103.7 \n",
      "Step: 200/256 Loss 1: 0.236 Loss 2: 0.096 Loss 3: 0.243 Sec: 173.1 \n",
      "Step: 100/1456 Loss 1: 0.237 Loss 2: 0.101 Loss 3: 0.391 Sec: 3.2 \n",
      "Step: 200/1456 Loss 1: 0.217 Loss 2: 0.103 Loss 3: 0.386 Sec: 4.8 \n",
      "Step: 300/1456 Loss 1: 0.219 Loss 2: 0.096 Loss 3: 0.320 Sec: 6.5 \n",
      "Step: 400/1456 Loss 1: 0.216 Loss 2: 0.095 Loss 3: 0.291 Sec: 8.1 \n",
      "Step: 500/1456 Loss 1: 0.211 Loss 2: 0.090 Loss 3: 0.273 Sec: 9.8 \n",
      "Step: 600/1456 Loss 1: 0.205 Loss 2: 0.087 Loss 3: 0.266 Sec: 11.2 \n",
      "Step: 700/1456 Loss 1: 0.200 Loss 2: 0.081 Loss 3: 0.244 Sec: 12.8 \n",
      "Step: 800/1456 Loss 1: 0.202 Loss 2: 0.083 Loss 3: 0.264 Sec: 14.1 \n",
      "Step: 900/1456 Loss 1: 0.199 Loss 2: 0.082 Loss 3: 0.259 Sec: 15.6 \n",
      "Step: 1000/1456 Loss 1: 0.203 Loss 2: 0.084 Loss 3: 0.261 Sec: 17.2 \n",
      "Step: 1100/1456 Loss 1: 0.204 Loss 2: 0.083 Loss 3: 0.262 Sec: 18.8 \n",
      "Step: 1200/1456 Loss 1: 0.208 Loss 2: 0.083 Loss 3: 0.257 Sec: 20.8 \n",
      "Step: 1300/1456 Loss 1: 0.210 Loss 2: 0.082 Loss 3: 0.253 Sec: 22.6 \n",
      "Step: 1400/1456 Loss 1: 0.209 Loss 2: 0.082 Loss 3: 0.257 Sec: 24.5 \n",
      "Epoch: 1/1 Sec: 224.4 \n",
      "BCE 1: 0.229 BCE 2: 0.093 BCE 3: 0.240 \n",
      "val BCE 1: 0.211 val BCE 2: 0.081 val BCE 3: 0.252 \n",
      "val bce negative_exam_for_pe: 0.381 val auc negative_exam_for_pe: 0.874 \n",
      "val bce indeterminate: 0.098 val auc indeterminate: 0.722 \n",
      "val bce chronic_pe: 0.161 val auc chronic_pe: 0.657 \n",
      "val bce acute_and_chronic_pe: 0.078 val auc acute_and_chronic_pe: 0.826 \n",
      "val bce central_pe: 0.117 val auc central_pe: 0.942 \n",
      "val bce leftsided_pe: 0.312 val auc leftsided_pe: 0.897 \n",
      "val bce rightsided_pe: 0.324 val auc rightsided_pe: 0.900 \n",
      "val bce rv_lv_ratio_gte_1: 0.241 val auc rv_lv_ratio_gte_1: 0.892 \n",
      "val bce rv_lv_ratio_lt_1: 0.393 val auc rv_lv_ratio_lt_1: 0.778 \n",
      "val bce pe_present_on_image: 0.078 val auc pe_present_on_image: 0.969 \n",
      "val bce q_pe_present_on_image: 0.227 val auc q_pe_present_on_image: 0.963 \n",
      "val loss : 0.219 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for fold in range(NUM_FOLD):\n",
    "    if fold+1 not in FOLD_LIST: continue\n",
    "    starttime = time.time()\n",
    "    print(\"fold: {}\".format(fold + 1))\n",
    "\n",
    "    # build model\n",
    "    if MODEL_NAME=='b0':\n",
    "        num_feature = 1280\n",
    "    elif MODEL_NAME=='b2':\n",
    "        num_feature = 1408\n",
    "    model = CNN_1D(num_classes=NUM_CLASS+1, input_ch=num_feature).cuda()\n",
    "    \n",
    "    # train dataset\n",
    "    X_train_exam = ri(df_train_exam[df_fold_exam['fold{}_train'.format(fold+1)]==1])\n",
    "    dataset_train = ImageDataset(X_train_exam, df_train, shuffle=True, \n",
    "                                 crop=128,\n",
    "                                 cropaug=32,\n",
    "                                 flip=0.5,\n",
    "                                 fold=fold+1,\n",
    "                                )\n",
    "    train_loader = DataLoader(dataset_train,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=multiprocessing.cpu_count(),\n",
    "                              pin_memory=True,\n",
    "                              worker_init_fn=lambda x: np.random.seed(),\n",
    "                              )\n",
    "\n",
    "    # valid dataset\n",
    "    X_valid_exam = ri(df_train_exam[df_fold_exam['fold{}_valid'.format(fold+1)]==1])\n",
    "    dataset_valid = ImageDataset(X_valid_exam, df_train, shuffle=False, crop=-1,fold=fold+1)\n",
    "    valid_loader = DataLoader(dataset_valid,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=multiprocessing.cpu_count(),\n",
    "                              pin_memory=True,\n",
    "                              )\n",
    "    \n",
    "    # set optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR_RANGE[0])\n",
    "    scheduler = CosineLR(optimizer, step_size_min=LR_RANGE[1], t0=STEP_PER_EPOCH * NUM_CYCLE, tmult=1)\n",
    "    \n",
    "    # set train log\n",
    "    train_log = pd.DataFrame(columns=log_columns)\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        loss1, loss2, loss3 = train(train_loader, model, optimizer, scheduler, num_step=STEP_PER_EPOCH, verbose=100)\n",
    "        val_loss1, val_loss2, val_loss3, val_pred, val_pred2, val_pred3, val_true, val_true2 = validate(valid_loader, model, verbose=100)\n",
    "\n",
    "        # record log\n",
    "        endtime = time.time() - starttime\n",
    "        print_log = \"Epoch: {}/{} \".format(epoch + 1, NUM_EPOCH)\n",
    "        print_log += \"Sec: {:.1f} \\n\".format(time.time()-starttime)\n",
    "        print_log += \"BCE 1: {:.3f} \".format(loss1)\n",
    "        print_log += \"BCE 2: {:.3f} \".format(loss2)\n",
    "        print_log += \"BCE 3: {:.3f} \\n\".format(loss3)\n",
    "        print_log += \"val BCE 1: {:.3f} \".format(val_loss1)\n",
    "        print_log += \"val BCE 2: {:.3f} \".format(val_loss2)\n",
    "        print_log += \"val BCE 3: {:.3f} \\n\".format(val_loss3)\n",
    "        train_log_epoch = [epoch+1, loss1, loss2, loss3, val_loss1, val_loss2, val_loss3, endtime]\n",
    "        \n",
    "        for i, col in enumerate(col_targets[:-1]):\n",
    "            score = metrics.log_loss(val_true[:,i], val_pred[:,i], labels=[0,1])\n",
    "            train_log_epoch.append(score)\n",
    "            print_log += \"val bce {}: {:.3f} \".format(col, score)\n",
    "            \n",
    "            score = metrics.roc_auc_score(val_true[:,i], val_pred[:,i])\n",
    "            train_log_epoch.append(score)\n",
    "            print_log += \"val auc {}: {:.3f} \\n\".format(col, score)\n",
    "        \n",
    "        score = metrics.log_loss(val_true2[:,0], val_pred2[:,0], labels=[0,1])\n",
    "        train_log_epoch.append(score)\n",
    "        print_log += \"val bce {}: {:.3f} \".format(col_targets[-1], score)\n",
    "\n",
    "        score = metrics.roc_auc_score(val_true2[:,0], val_pred2[:,0])\n",
    "        train_log_epoch.append(score)\n",
    "        print_log += \"val auc {}: {:.3f} \\n\".format(col_targets[-1], score)\n",
    "        \n",
    "        pred_tmp = np.clip(val_pred3[:,0], 1e-15, 1 - 1e-15)\n",
    "        true_tmp = df_train['pe_present_on_image'].values[df_fold['fold{}_valid'.format(fold+1)]==1]\n",
    "        q_tmp = df_train['q_i'].values[df_fold['fold{}_valid'.format(fold+1)]==1]\n",
    "        tmp = -q_tmp*(np.log(pred_tmp)*true_tmp + np.log(1-pred_tmp)*(1-true_tmp))\n",
    "        score = np.sum(tmp)/np.sum(q_tmp)\n",
    "        train_log_epoch.append(score)\n",
    "        print_log += \"val bce q_{}: {:.3f} \".format(col_targets[-1], score)\n",
    "        val_loss = (val_loss1 + score)/2\n",
    "\n",
    "        score = metrics.roc_auc_score(val_true2[:,0], val_pred3[:,0])\n",
    "        train_log_epoch.append(score)\n",
    "        print_log += \"val auc q_{}: {:.3f} \\n\".format(col_targets[-1], score)\n",
    "        \n",
    "        train_log_epoch.append(val_loss)\n",
    "        print_log += \"val loss : {:.3f} \\n\".format(val_loss)\n",
    "        \n",
    "        train_log_epoch = pd.DataFrame([train_log_epoch], columns=log_columns)\n",
    "        train_log = pd.concat([train_log, train_log_epoch])\n",
    "        train_log.to_csv(\"{}/1dcnn_train_log_{}_fold{}.csv\".format(output_dir, MODEL_NAME, fold + 1), index=False)\n",
    "\n",
    "        # display log\n",
    "        print(print_log)\n",
    "        \n",
    "        # save weights\n",
    "        if val_loss==train_log['val_loss'].min():\n",
    "            torch.save(model.state_dict(), \"{}/1dcnn_weight_{}_best_fold{}.pth\".format(\n",
    "                output_dir, MODEL_NAME, fold + 1))\n",
    "        if (epoch+1)%NUM_CYCLE==0:\n",
    "            torch.save(model.state_dict(), \"{}/1dcnn_weight_{}_epoch_{}_fold{}.pth\".format(\n",
    "                    output_dir, MODEL_NAME, epoch+1, fold + 1))\n",
    "            torch.save(optimizer.state_dict(), \"{}/1dcnn_optimizer_{}_epoch_{}_fold{}.pth\".format(\n",
    "                    output_dir, MODEL_NAME, epoch+1, fold + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, model, verbose=100):\n",
    "    lastfunc = nn.Sigmoid()\n",
    "\n",
    "    # switch to eval mode\n",
    "    model .eval()\n",
    "\n",
    "    starttime = time.time()\n",
    "    # training\n",
    "    preds = np.zeros([0, NUM_CLASS-1], np.float32)\n",
    "    preds2 = np.zeros([0, 1], np.float32)\n",
    "    preds3 = np.zeros([0, 1], np.float32)\n",
    "    for i, (input1, (_, _, _, num_series)) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            input1 = torch.autograd.Variable(input1.cuda())\n",
    "            # compute output\n",
    "            output1, output23 = model(input1)\n",
    "            output2 = output23[:,:-1]\n",
    "            output3 = output23[:,-1:]\n",
    "            output1 = lastfunc(output1)\n",
    "            output2 = lastfunc(output2)[:,:,:num_series]\n",
    "            output3 = lastfunc(output3)[:,:,:num_series]\n",
    "        \n",
    "        # record pred\n",
    "        preds = np.concatenate([preds, output1.data.cpu().numpy()])\n",
    "        preds2 = np.concatenate([preds2, output2.data.cpu().numpy().reshape([-1, 1])])\n",
    "        preds3 = np.concatenate([preds3, output3.data.cpu().numpy().reshape([-1, 1])])\n",
    "        \n",
    "        # display log\n",
    "        if (i+1)%verbose==0:\n",
    "            print(\"Step: {}/{} \".format(i + 1, len(loader))\n",
    "                  + \"Sec: {:.1f} \".format(time.time()-starttime)\n",
    "                  )\n",
    "        \n",
    "    return preds, preds2, preds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulid model\n",
    "if MODEL_NAME=='b0':\n",
    "    num_feature = 1280\n",
    "elif MODEL_NAME=='b2':\n",
    "    num_feature = 1408\n",
    "model = CNN_1D(num_classes=NUM_CLASS+1, input_ch=num_feature).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100/1456 Sec: 0.9 \n",
      "Step: 200/1456 Sec: 1.5 \n",
      "Step: 300/1456 Sec: 2.1 \n",
      "Step: 400/1456 Sec: 2.8 \n",
      "Step: 500/1456 Sec: 3.4 \n",
      "Step: 600/1456 Sec: 4.1 \n",
      "Step: 700/1456 Sec: 4.7 \n",
      "Step: 800/1456 Sec: 5.4 \n",
      "Step: 900/1456 Sec: 6.0 \n",
      "Step: 1000/1456 Sec: 6.6 \n",
      "Step: 1100/1456 Sec: 7.3 \n",
      "Step: 1200/1456 Sec: 7.9 \n",
      "Step: 1300/1456 Sec: 8.6 \n",
      "Step: 1400/1456 Sec: 9.3 \n"
     ]
    }
   ],
   "source": [
    "for fold in range(NUM_FOLD):\n",
    "    if fold+1 not in FOLD_LIST: continue\n",
    "    # valid dataset\n",
    "    X_valid_study = ri(df_train_exam[df_fold_exam['fold{}_valid'.format(fold+1)]==1])\n",
    "    dataset_valid = ImageDataset(X_valid_study, df_train, shuffle=False, crop=-1, fold=fold+1)\n",
    "    valid_loader = DataLoader(dataset_valid,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              num_workers=multiprocessing.cpu_count(),\n",
    "                              pin_memory=True,\n",
    "                              )\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"{}/1dcnn_weight_{}_best_fold{}.pth\".format(\n",
    "                    output_dir, MODEL_NAME, epoch+1, fold + 1)))\n",
    "    preds_valid1, preds_valid2, preds_valid3 = predict(valid_loader, model, verbose=100)\n",
    "    np.save(\"{}/1dcnn_preds_{}_valid1_fold{}.npy\".format(output_dir, MODEL_NAME, fold+1), preds_valid1)\n",
    "    np.save(\"{}/1dcnn_preds_{}_valid2_fold{}.npy\".format(output_dir, MODEL_NAME, fold+1), preds_valid2)\n",
    "    np.save(\"{}/1dcnn_preds_{}_valid3_fold{}.npy\".format(output_dir, MODEL_NAME, fold+1), preds_valid3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
